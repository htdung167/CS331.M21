{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eed34648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de21cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "  def cost_function(self, X, y, theta):\n",
    "    m = len(y)\n",
    "    y_pred = X@theta\n",
    "    return (1/(2*m)) * np.sum((y_pred-y)**2)\n",
    "  \n",
    "  def gradient_descent(self, X, y, theta, alpha, iters):\n",
    "    m = len(y)\n",
    "    costs = []\n",
    "    for i in range(iters):\n",
    "      y_pred = X@theta\n",
    "      theta -= (alpha/m * (X.T@(y_pred-y))) #Vector h√≥a\n",
    "      costs.append(self.cost_func(X, y,theta))\n",
    "    return theta, costs\n",
    "\n",
    "  def fit(self, X, y, iters=15, learning_rate=0.01):\n",
    "    m = len(y)\n",
    "    n = len(X[0])\n",
    "    self.X_train = np.append(np.ones(m,1), X.copy(), axis=1) #X: mxn\n",
    "    self.y_train = y #y : mx1\n",
    "    self.theta = np.zeros((n + 1, 1)) # theta = nx1\n",
    "    self.theta, self.costs = self.gradient_descent(self.X_train, self.y_train, self.theta, iters, learning_rate)\n",
    "    return self.theta, self.costs\n",
    "  \n",
    "  def predict(self, X_pred):\n",
    "    X_pred_t = np.append(np.ones(len(X_pred), 1), X_pred, axis=1)\n",
    "    return X_pred_t@self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff64caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "  def __init__():\n",
    "    pass\n",
    "\n",
    "  def sigmoid(self, z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "  def cost_function_1(self, X, y, theta):\n",
    "    m = len(y)\n",
    "    y_pred = self.sigmoid(X@theta)\n",
    "    cost = -1.0/m * (y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
    "    return cost\n",
    "\n",
    "  def gradient_descent_1(self, X, y, theta, alpha, iters):\n",
    "    m = len(y)\n",
    "    costs = []\n",
    "    for i in range(iters):\n",
    "      h = self.cost_function_1(X, y, theta)\n",
    "      gradient = X.T@(h-y) \n",
    "      theta -= alpha / m * gradient \n",
    "      costs.append(self.cost_function_1(X, y, theta))\n",
    "    return theta, costs\n",
    "\n",
    "  def fit(self, X, y, iters=15, learning_rate=0.01, loss=\"crossentropy\"):\n",
    "    m = len(y)\n",
    "    n = len(X[0])\n",
    "    self.X_train = np.append(np.ones(m,1), X.copy(), axis=1) #X: mxn\n",
    "    self.y_train = y #y : mx1\n",
    "    self.theta = np.zeros((n + 1, 1)) # theta = nx1\n",
    "    if loss == \"crossentropy\":\n",
    "      self.theta, self.costs = self.gradient_descent_1(self.X_train, self.y_train, self.theta, iters, learning_rate)\n",
    "    elif loss == 'mse':\n",
    "      pass\n",
    "    return self.theta, self.costs\n",
    "\n",
    "  def predict(self, X_pred):\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
