{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CaiDatLinearVaLogistic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Thông tin sinh viên:**"
      ],
      "metadata": {
        "id": "BDbH-ypUcuL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* MSSV: 19521388\n",
        "* Họ và tên: Hoàng Tiến Dũng\n",
        "* Lớp môn học: CS331.M11\n",
        "* Bài tập: Cài đặt Linear và Logistic"
      ],
      "metadata": {
        "id": "QRc8rQSScwRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Bài làm:**"
      ],
      "metadata": {
        "id": "QhW-BevNc_Ke"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ANlWCRRWcoB7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**I. Linear Regression**"
      ],
      "metadata": {
        "id": "0RSQi-8kdDxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def cost_function(self, X, y, theta):\n",
        "    m = len(y)\n",
        "    y_pred = X@theta\n",
        "    return (1/(2*m)) * np.sum((y_pred-y)**2)\n",
        "  \n",
        "  def gradient_descent(self, X, y, theta, alpha, iters):\n",
        "    m = len(y)\n",
        "    costs = []\n",
        "    for i in range(iters):\n",
        "      y_pred = X@theta\n",
        "      theta -= (alpha/m * (X.T@(y_pred-y))) #Vector hóa\n",
        "      costs.append(self.cost_func(X, y,theta))\n",
        "    return theta, costs\n",
        "\n",
        "  def fit(self, X, y, iters=15, learning_rate=0.01):\n",
        "    m = len(y)\n",
        "    n = len(X[0])\n",
        "    self.X_train = np.append(np.ones(m,1), X.copy(), axis=1) #X: mxn\n",
        "    self.y_train = y #y : mx1\n",
        "    self.theta = np.zeros((n + 1, 1)) # theta = nx1\n",
        "    self.theta, self.costs = self.gradient_descent(self.X_train, self.y_train, self.theta, iters, learning_rate)\n",
        "    return self.theta, self.costs\n",
        "  \n",
        "  def predict(self, X_pred):\n",
        "    X_pred_t = np.append(np.ones(len(X_pred), 1), X_pred, axis=1)\n",
        "    return X_pred_t@self.theta"
      ],
      "metadata": {
        "id": "0ZVGRIsjdA9E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**II. Logistic Regression**"
      ],
      "metadata": {
        "id": "hzIyuh-bdhxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression():\n",
        "  def __init__():\n",
        "    pass\n",
        "\n",
        "  def sigmoid(self, z):\n",
        "    return 1.0/(1.0 + np.exp(-z))\n",
        "\n",
        "  def cost_function_1(self, X, y, theta): #cross entropy\n",
        "    m = len(y)\n",
        "    y_pred = self.sigmoid(X@theta)\n",
        "    cost = -1.0/m * (y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
        "    return cost\n",
        "\n",
        "  def gradient_descent_1(self, X, y, theta, alpha, iters):\n",
        "    m = len(y)\n",
        "    costs = []\n",
        "    for i in range(iters):\n",
        "      h = self.cost_function_1(X, y, theta)\n",
        "      gradient = X.T@(h-y) \n",
        "      theta -= alpha / m * gradient \n",
        "      costs.append(self.cost_function_1(X, y, theta))\n",
        "    return theta, costs\n",
        "\n",
        "  def cost_function_2(self, X, y, theta): #mean squared error\n",
        "    m = len(y)\n",
        "    y_pred = self.sigmoid(X@theta)\n",
        "    return (1/(2*m)) * np.sum((y_pred-y)**2)\n",
        "\n",
        "  def gradient_descent_2(self, X, y, theta, alpha, iters):\n",
        "    m = len(y)\n",
        "    costs = []\n",
        "    for i in range(iters):\n",
        "      y_pred = self.sigmoid(X@theta)\n",
        "      theta -= (alpha/m * (X.T@(y_pred-y)))\n",
        "      costs.append(self.cost_func(X, y,theta))\n",
        "    return theta, costs\n",
        "\n",
        "  def fit(self, X, y, iters=15, learning_rate=0.01, loss=\"crossentropy\"):\n",
        "    m = len(y)\n",
        "    n = len(X[0])\n",
        "    self.X_train = np.append(np.ones(m,1), X.copy(), axis=1) #X: mxn\n",
        "    self.y_train = y #y : mx1\n",
        "    self.theta = np.zeros((n + 1, 1)) # theta = nx1\n",
        "    if loss == \"crossentropy\":\n",
        "      self.theta, self.costs = self.gradient_descent_1(self.X_train, self.y_train, self.theta, iters, learning_rate)\n",
        "    elif loss == 'mse':\n",
        "      self.theta, self.costs = self.gradient_descent_2(self.X_train, self.y_train, self.theta, iters, learning_rate)\n",
        "    return self.theta, self.costs\n",
        "\n",
        "  def predict(self, X_pred):\n",
        "    X_pred_t = np.append(np.ones(len(X_pred), 1), X_pred, axis=1)\n",
        "    return X_pred_t@self.theta\n"
      ],
      "metadata": {
        "id": "W2gXwqFZdlDv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}