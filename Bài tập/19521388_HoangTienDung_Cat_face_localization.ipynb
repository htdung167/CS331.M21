{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0_fYrhMDOI2"
      },
      "source": [
        "#BƯỚC 1: KẾT NỐI VỚI GOOGLE DRIVE\n",
        "\n",
        "Để có thể load dữ liệu ảnh, lưu file model sau khi huấn luyện, ta phải cài đặt thư viện kết nối Colab với Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TCqE_aaBGdh",
        "outputId": "5c882747-529d-43d0-cb66-22bc41927a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzDO8POdDuGm"
      },
      "source": [
        "# BƯỚC 2: CHUẨN BỊ DỮ LIỆU HUẤN LUYỆN\n",
        "\n",
        "Bài tập này chúng ta sẽ tiến hành huấn luyện module xác định bounding box khoanh vùng mặt của một con mèo (Cat Face). Tập dữ liệu Cat Dataset bao gồm khoảng 10.000 được download tại trang web sau: https://web.archive.org/web/20150520175645/http://137.189.35.203/WebUI/CatDatabase/catData.html\n",
        "\n",
        "Dưới đây là một số ảnh nằm trong dataset này.\n",
        "\n",
        "![Một số ảnh mèo trong dataset](https://web.archive.org/web/20150520175645im_/http://137.189.35.203/WebUI/CatDatabase/Samples.jpg)\n",
        "\n",
        "**Annotation:**\n",
        "Dataset gồm có 7 thư mục ảnh 'CAT_00' đến 'CAT_06'. Trong các thư mục ảnh này sẽ có một cặp file có cùng tên khác phần mở rộng tương ứng là ảnh gốc và file chứa các điểm gán nhãn. Ví dụ như cặp file (00000003_015.jpg , 00000003_015.jpg.cat). File .cat sẽ có 9 cặp điểm tương ứng với các bộ phận trên mặt của một con mèo như: đỉnh tai, mắt, mũi, miệng, ...\n",
        "Lưu ý rằng trong bài toán phát hiện mặt mèo này, chúng ta không cần thông tin của các đỉnh này mà chỉ cần quan tâm đến đường bao (bounding box) xung quanh mặt của một con mèo được tạo dựa trên các đỉnh này.\n",
        "![cat annotation](https://web.archive.org/web/20150520175645im_/http://137.189.35.203/WebUI/CatDatabase/annotation.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bmg69XAPAwYa"
      },
      "outputs": [],
      "source": [
        "# !wget https://web.archive.org/web/20150520175645/http://137.189.35.203/WebUI/CatDatabase/Data/CAT_DATASET_01.zip\n",
        "# !unzip CAT_DATASET_01.zip\n",
        "# !wget https://web.archive.org/web/20150520175645/http://137.189.35.203/WebUI/CatDatabase/Data/CAT_DATASET_02.zip\n",
        "# !unzip CAT_DATASET_02.zip\n",
        "# !wget https://web.archive.org/web/20150520175645/http://137.189.35.203/WebUI/CatDatabase/Data/00000003_015.jpg.cat\n",
        "# !mv 00000003_015.jpg.cat \"./CAT_00\"\n",
        "# !mv CAT_* \"/content/gdrive/MyDrive/Private_ThiGiacMayTinhNangCao/CatData\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl4mTJTOHgHG"
      },
      "source": [
        "Kiểm tra xem trong thư mục đã có các thư mục dataset chưa?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y1Kdw_AHqpz"
      },
      "source": [
        "# BƯỚC 3: KHỞI TẠO THAM SỐ VÀ ĐƯỜNG DẪN\n",
        "\n",
        "Trong chương trình này chúng ta sẽ sẽ dụng Keras để cài đặt với các tham số mặc định như bên dưới."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy==1.1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN2i-pXbjseA",
        "outputId": "88a7f07a-00ff-4667-ab59-b42c5cd9bb46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy==1.1.0 in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.1.0) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4OR-t2pjeIe",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "56d669e4-4341-44e0-f4da-c7f210d12de0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-eb159a10-5148-4279-bbe1-b43faa2a206c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-eb159a10-5148-4279-bbe1-b43faa2a206c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dataset.py to dataset.py\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9b53bb40-c6ea-4732-b53f-18dd2889a11a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9b53bb40-c6ea-4732-b53f-18dd2889a11a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving split_by_color.py to split_by_color.py\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-424be687-b43b-484d-bc16-264e0c643631\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-424be687-b43b-484d-bc16-264e0c643631\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ImageAugmenter.py to ImageAugmenter.py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ImageAugmenter.py': b'# -*- coding: utf-8 -*-\\n\"\"\"Wrapper functions and classes around scikit-images AffineTransformation.\\nSimplifies augmentation of images in machine learning.\\n\\nExample usage:\\n        img_width = 32 # width of the images\\n        img_height = 32 # height of the images\\n        images = ... # e.g. load via scipy.misc.imload(filename)\\n\\n        # For each image: randomly flip it horizontally (50% chance),\\n        # randomly rotate it between -20 and +20 degrees, randomly translate\\n        # it on the x-axis between -5 and +5 pixel.\\n        ia = ImageAugmenter(img_width, img_height, hlip=True, rotation_deg=20,\\n                            translation_x_px=5)\\n        augmented_images = ia.augment_batch(images)\\n\"\"\"\\nfrom __future__ import division\\nfrom skimage import transform as tf\\nimport numpy as np\\nimport random\\n\\ndef is_minmax_tuple(param):\\n    \"\"\"Returns whether the parameter is a tuple containing two values.\\n\\n    Used in create_aug_matrices() and probably useless everywhere else.\\n\\n    Args:\\n        param: The parameter to check (whether it is a tuple of length 2).\\n\\n    Returns:\\n        Boolean\\n    \"\"\"\\n    return type(param) is tuple and len(param) == 2\\n\\ndef create_aug_matrices(nb_matrices, img_width_px, img_height_px,\\n                        scale_to_percent=1.0, scale_axis_equally=False,\\n                        rotation_deg=0, shear_deg=0,\\n                        translation_x_px=0, translation_y_px=0,\\n                        seed=None):\\n    \"\"\"Creates the augmentation matrices that may later be used to transform\\n    images.\\n\\n    This is a wrapper around scikit-image\\'s transform.AffineTransform class.\\n    You can apply those matrices to images using the apply_aug_matrices()\\n    function.\\n\\n    Args:\\n        nb_matrices: How many matrices to return, e.g. 100 returns 100 different\\n            random-generated matrices (= 100 different transformations).\\n        img_width_px: Width of the images that will be transformed later\\n            on (same as the width of each of the matrices).\\n        img_height_px: Height of the images that will be transformed later\\n            on (same as the height of each of the matrices).\\n        scale_to_percent: Same as in ImageAugmenter.__init__().\\n            Up to which percentage the images may be\\n            scaled/zoomed. The negative scaling is automatically derived\\n            from this value. A value of 1.1 allows scaling by any value\\n            between -10% and +10%. You may set min and max values yourself\\n            by using a tuple instead, like (1.1, 1.2) to scale between\\n            +10% and +20%. Default is 1.0 (no scaling).\\n        scale_axis_equally: Same as in ImageAugmenter.__init__().\\n            Whether to always scale both axis (x and y)\\n            in the same way. If set to False, then e.g. the Augmenter\\n            might scale the x-axis by 20% and the y-axis by -5%.\\n            Default is False.\\n        rotation_deg: Same as in ImageAugmenter.__init__().\\n            By how much the image may be rotated around its\\n            center (in degrees). The negative rotation will automatically\\n            be derived from this value. E.g. a value of 20 allows any\\n            rotation between -20 degrees and +20 degrees. You may set min\\n            and max values yourself by using a tuple instead, e.g. (5, 20)\\n            to rotate between +5 und +20 degrees. Default is 0 (no\\n            rotation).\\n        shear_deg: Same as in ImageAugmenter.__init__().\\n            By how much the image may be sheared (in degrees). The\\n            negative value will automatically be derived from this value.\\n            E.g. a value of 20 allows any shear between -20 degrees and\\n            +20 degrees. You may set min and max values yourself by using a\\n            tuple instead, e.g. (5, 20) to shear between +5 und +20\\n            degrees. Default is 0 (no shear).\\n        translation_x_px: Same as in ImageAugmenter.__init__().\\n            By up to how many pixels the image may be\\n            translated (moved) on the x-axis. The negative value will\\n            automatically be derived from this value. E.g. a value of +7\\n            allows any translation between -7 and +7 pixels on the x-axis.\\n            You may set min and max values yourself by using a tuple\\n            instead, e.g. (5, 20) to translate between +5 und +20 pixels.\\n            Default is 0 (no translation on the x-axis).\\n        translation_y_px: Same as in ImageAugmenter.__init__().\\n            See translation_x_px, just for the y-axis.\\n        seed: Seed to use for python\\'s and numpy\\'s random functions.\\n\\n    Returns:\\n        List of augmentation matrices.\\n    \"\"\"\\n    assert nb_matrices > 0\\n    assert img_width_px > 0\\n    assert img_height_px > 0\\n    assert is_minmax_tuple(scale_to_percent) or scale_to_percent >= 1.0\\n    assert is_minmax_tuple(rotation_deg) or rotation_deg >= 0\\n    assert is_minmax_tuple(shear_deg) or shear_deg >= 0\\n    assert is_minmax_tuple(translation_x_px) or translation_x_px >= 0\\n    assert is_minmax_tuple(translation_y_px) or translation_y_px >= 0\\n\\n    if seed is not None:\\n        random.seed(seed)\\n        np.random.seed(seed)\\n\\n    result = []\\n\\n    shift_x = int(img_width_px / 2.0)\\n    shift_y = int(img_height_px / 2.0)\\n\\n    # prepare min and max values for\\n    # scaling/zooming (min/max values)\\n    if is_minmax_tuple(scale_to_percent):\\n        scale_x_min = scale_to_percent[0]\\n        scale_x_max = scale_to_percent[1]\\n    else:\\n        scale_x_min = scale_to_percent\\n        scale_x_max = 1.0 - (scale_to_percent - 1.0)\\n    assert scale_x_min > 0.0\\n    #if scale_x_max >= 2.0:\\n    #     warnings.warn(\"Scaling by more than 100 percent (%.2f).\" % (scale_x_max,))\\n    scale_y_min = scale_x_min # scale_axis_equally affects the random value generation\\n    scale_y_max = scale_x_max\\n\\n    # rotation (min/max values)\\n    if is_minmax_tuple(rotation_deg):\\n        rotation_deg_min = rotation_deg[0]\\n        rotation_deg_max = rotation_deg[1]\\n    else:\\n        rotation_deg_min = (-1) * int(rotation_deg)\\n        rotation_deg_max = int(rotation_deg)\\n\\n    # shear (min/max values)\\n    if is_minmax_tuple(shear_deg):\\n        shear_deg_min = shear_deg[0]\\n        shear_deg_max = shear_deg[1]\\n    else:\\n        shear_deg_min = (-1) * int(shear_deg)\\n        shear_deg_max = int(shear_deg)\\n\\n    # translation x-axis (min/max values)\\n    if is_minmax_tuple(translation_x_px):\\n        translation_x_px_min = translation_x_px[0]\\n        translation_x_px_max = translation_x_px[1]\\n    else:\\n        translation_x_px_min = (-1) * translation_x_px\\n        translation_x_px_max = translation_x_px\\n\\n    # translation y-axis (min/max values)\\n    if is_minmax_tuple(translation_y_px):\\n        translation_y_px_min = translation_y_px[0]\\n        translation_y_px_max = translation_y_px[1]\\n    else:\\n        translation_y_px_min = (-1) * translation_y_px\\n        translation_y_px_max = translation_y_px\\n\\n    # create nb_matrices randomized affine transformation matrices\\n    for _ in range(nb_matrices):\\n        # generate random values for scaling, rotation, shear, translation\\n        scale_x = random.uniform(scale_x_min, scale_x_max)\\n        scale_y = random.uniform(scale_y_min, scale_y_max)\\n        if not scale_axis_equally:\\n            scale_y = random.uniform(scale_y_min, scale_y_max)\\n        else:\\n            scale_y = scale_x\\n        rotation = np.deg2rad(random.randint(rotation_deg_min, rotation_deg_max))\\n        shear = np.deg2rad(random.randint(shear_deg_min, shear_deg_max))\\n        translation_x = random.randint(translation_x_px_min, translation_x_px_max)\\n        translation_y = random.randint(translation_y_px_min, translation_y_px_max)\\n\\n        # create three affine transformation matrices\\n        # 1st one moves the image to the top left, 2nd one transforms it, 3rd one\\n        # moves it back to the center.\\n        # The movement is neccessary, because rotation is applied to the top left\\n        # and not to the image\\'s center (same for scaling and shear).\\n        matrix_to_topleft = tf.SimilarityTransform(translation=[-shift_x, -shift_y])\\n        matrix_transforms = tf.AffineTransform(scale=(scale_x, scale_y),\\n                                               rotation=rotation, shear=shear,\\n                                               translation=(translation_x,\\n                                                            translation_y))\\n        matrix_to_center = tf.SimilarityTransform(translation=[shift_x, shift_y])\\n\\n        # Combine the three matrices to one affine transformation (one matrix)\\n        matrix = matrix_to_topleft + matrix_transforms + matrix_to_center\\n\\n        # one matrix is ready, add it to the result\\n        result.append(matrix.inverse)\\n\\n    return result\\n\\ndef apply_aug_matrices(images, matrices, transform_channels_equally=True,\\n                       channel_is_first_axis=False, random_order=True,\\n                       mode=\"constant\", cval=0.0, interpolation_order=1,\\n                       seed=None):\\n    \"\"\"Augment the given images using the given augmentation matrices.\\n\\n    This function is a wrapper around scikit-image\\'s transform.warp().\\n    It is expected to be called by ImageAugmenter.augment_batch().\\n    The matrices may be generated by create_aug_matrices().\\n\\n    Args:\\n        images: Same as in ImageAugmenter.augment_batch().\\n            Numpy array (dtype: uint8, i.e. values 0-255) with the images.\\n            Expected shape is either (image-index, height, width) for\\n            grayscale images or (image-index, channel, height, width) for\\n            images with channels (e.g. RGB) where the channel has the first\\n            index or (image-index, height, width, channel) for images with\\n            channels, where the channel is the last index.\\n            If your shape is (image-index, channel, width, height) then\\n            you must also set channel_is_first_axis=True in the constructor.\\n        matrices: A list of augmentation matrices as produced by\\n            create_aug_matrices().\\n        transform_channels_equally: Same as in ImageAugmenter.__init__().\\n            Whether to apply the exactly same\\n            transformations to each channel of an image (True). Setting\\n            it to False allows different transformations per channel,\\n            e.g. the red-channel might be rotated by +20 degrees, while\\n            the blue channel (of the same image) might be rotated\\n            by -5 degrees. If you don\\'t have any channels (2D grayscale),\\n            you can simply ignore this setting.\\n            Default is True (transform all equally).\\n        channel_is_first_axis: Same as in ImageAugmenter.__init__().\\n            Whether the channel (e.g. RGB) is the first\\n            axis of each image (True) or the last axis (False).\\n            False matches the scipy and PIL implementation and is the\\n            default. If your images are 2D-grayscale then you can ignore\\n            this setting (as the augmenter will ignore it too).\\n        random_order: Whether to apply the augmentation matrices in a random\\n            order (True, e.g. the 2nd matrix might be applied to the\\n            5th image) or in the given order (False, e.g. the 2nd matrix might\\n            be applied to the 2nd image).\\n            Notice that for multi-channel images (e.g. RGB) this function\\n            will use a different matrix for each channel, unless\\n            transform_channels_equally is set to True.\\n        mode: Parameter used for the transform.warp-function of scikit-image.\\n            Can usually be ignored.\\n        cval: Parameter used for the transform.warp-function of scikit-image.\\n            Defines the fill color for \"new\" pixels, e.g. for empty areas\\n            after rotations. (0.0 is black, 1.0 is white.)\\n        interpolation_order: Parameter used for the transform.warp-function of\\n            scikit-image. Defines the order of all interpolations used to\\n            generate the new/augmented image. See their documentation for\\n            further details.\\n        seed: Seed to use for python\\'s and numpy\\'s random functions.\\n    \"\"\"\\n    # images must be numpy array\\n    assert type(images).__module__ == np.__name__, \"Expected numpy array for \" \\\\\\n                                                   \"parameter \\'images\\'.\"\\n\\n    # images must have uint8 as dtype (0-255)\\n    assert images.dtype.name == \"uint8\", \"Expected numpy.uint8 as image dtype.\"\\n\\n    # 3 axis total (2 per image) for grayscale,\\n    # 4 axis total (3 per image) for RGB (usually)\\n    assert len(images.shape) in [3, 4], \"\"\"Expected \\'images\\' parameter to have\\n        either shape (image index, y, x) for greyscale\\n        or (image index, channel, y, x) / (image index, y, x, channel)\\n        for multi-channel (usually color) images.\"\"\"\\n\\n    if seed:\\n        np.random.seed(seed)\\n\\n    nb_images = images.shape[0]\\n\\n    # estimate number of channels, set to 1 if there is no axis channel,\\n    # otherwise it will usually be 3\\n    has_channels = False\\n    nb_channels = 1\\n    if len(images.shape) == 4:\\n        has_channels = True\\n        if channel_is_first_axis:\\n            nb_channels = images.shape[1] # first axis within each image\\n        else:\\n            nb_channels = images.shape[3] # last axis within each image\\n\\n    # whether to apply the transformations directly to the whole image\\n    # array (True) or for each channel individually (False)\\n    apply_directly = not has_channels or (transform_channels_equally\\n                                          and not channel_is_first_axis)\\n\\n    # We generate here the order in which the matrices may be applied.\\n    # At the end, order_indices will contain the index of the matrix to use\\n    # for each image, e.g. [15, 2] would mean, that the 15th matrix will be\\n    # applied to the 0th image, the 2nd matrix to the 1st image.\\n    # If the images gave multiple channels (e.g. RGB) and\\n    # transform_channels_equally has been set to False, we will need one\\n    # matrix per channel instead of per image.\\n\\n    # 0 to nb_images, but restart at 0 if index is beyond number of matrices\\n    len_indices = nb_images if apply_directly else nb_images * nb_channels\\n    if random_order:\\n        # Notice: This way to choose random matrices is concise, but can create\\n        # problems if there is a low amount of images and matrices.\\n        # E.g. suppose that 2 images are ought to be transformed by either\\n        # 0px translation on the x-axis or 1px translation. So 50% of all\\n        # matrices translate by 0px and 50% by 1px. The following method\\n        # will randomly choose a combination of the two matrices for the\\n        # two images (matrix 0 for image 0 and matrix 0 for image 1,\\n        # matrix 0 for image 0 and matrix 1 for image 1, ...).\\n        # In 50% of these cases, a different matrix will be chosen for image 0\\n        # and image 1 (matrices 0, 1 or matrices 1, 0). But 50% of these\\n        # \"different\" matrices (different index) will be the same, as 50%\\n        # translate by 1px and 50% by 0px. As a result, 75% of all augmentations\\n        # will transform both images in the same way.\\n        # The effect decreases if more matrices or images are chosen.\\n        order_indices = np.random.random_integers(0, len(matrices) - 1, len_indices)\\n    else:\\n        # monotonously growing indexes (each by +1), but none of them may be\\n        # higher than or equal to the number of matrices\\n        order_indices = np.arange(0, len_indices) % len(matrices)\\n\\n    result = np.zeros(images.shape, dtype=np.float32)\\n    matrix_number = 0\\n\\n    # iterate over every image, find out which matrix to apply and then use\\n    # that matrix to augment the image\\n    for img_idx, image in enumerate(images):\\n        if apply_directly:\\n            # we can apply the matrix to the whole numpy array of the image\\n            # at the same time, so we do that to save time (instead of eg. three\\n            # steps for three channels as in the else-part)\\n            matrix = matrices[order_indices[matrix_number]]\\n            result[img_idx, ...] = tf.warp(image, matrix, mode=mode, cval=cval,\\n                                           order=interpolation_order)\\n            matrix_number += 1\\n        else:\\n            # we cant apply the matrix to the whole image in one step, instead\\n            # we have to apply it to each channel individually. that happens\\n            # if the channel is the first axis of each image (incompatible with\\n            # tf.warp()) or if it was explicitly requested via\\n            # transform_channels_equally=False.\\n            for channel_idx in range(nb_channels):\\n                matrix = matrices[order_indices[matrix_number]]\\n                if channel_is_first_axis:\\n                    warped = tf.warp(image[channel_idx], matrix, mode=mode,\\n                                     cval=cval, order=interpolation_order)\\n                    result[img_idx, channel_idx, ...] = warped\\n                else:\\n                    warped = tf.warp(image[..., channel_idx], matrix, mode=mode,\\n                                     cval=cval, order=interpolation_order)\\n                    result[img_idx, ..., channel_idx] = warped\\n\\n                if not transform_channels_equally:\\n                    matrix_number += 1\\n            if transform_channels_equally:\\n                matrix_number += 1\\n\\n    return result\\n\\nclass ImageAugmenter(object):\\n    \"\"\"Helper class to randomly augment images, usually for neural networks.\\n\\n    Example usage:\\n        img_width = 32 # width of the images\\n        img_height = 32 # height of the images\\n        images = ... # e.g. load via scipy.misc.imload(filename)\\n\\n        # For each image: randomly flip it horizontally (50% chance),\\n        # randomly rotate it between -20 and +20 degrees, randomly translate\\n        # it on the x-axis between -5 and +5 pixel.\\n        ia = ImageAugmenter(img_width, img_height, hlip=True, rotation_deg=20,\\n                            translation_x_px=5)\\n        augmented_images = ia.augment_batch(images)\\n    \"\"\"\\n    def __init__(self, img_width_px, img_height_px, channel_is_first_axis=False,\\n                 hflip=False, vflip=False,\\n                 scale_to_percent=1.0, scale_axis_equally=False,\\n                 rotation_deg=0, shear_deg=0,\\n                 translation_x_px=0, translation_y_px=0,\\n                 transform_channels_equally=True):\\n        \"\"\"\\n        Args:\\n            img_width_px: The intended width of each image in pixels.\\n            img_height_px: The intended height of each image in pixels.\\n            channel_is_first_axis: Whether the channel (e.g. RGB) is the first\\n                axis of each image (True) or the last axis (False).\\n                False matches the scipy and PIL implementation and is the\\n                default. If your images are 2D-grayscale then you can ignore\\n                this setting (as the augmenter will ignore it too).\\n            hflip: Whether to randomly flip images horizontally (on the y-axis).\\n                You may choose either False (no horizontal flipping),\\n                True (flip with probability 0.5) or use a float\\n                value (probability) between 0.0 and 1.0. Default is False.\\n            vflip: Whether to randomly flip images vertically (on the x-axis).\\n                You may choose either False (no vertical flipping),\\n                True (flip with probability 0.5) or use a float\\n                value (probability) between 0.0 and 1.0. Default is False.\\n            scale_to_percent: Up to which percentage the images may be\\n                scaled/zoomed. The negative scaling is automatically derived\\n                from this value. A value of 1.1 allows scaling by any value\\n                between -10% and +10%. You may set min and max values yourself\\n                by using a tuple instead, like (1.1, 1.2) to scale between\\n                +10% and +20%. Default is 1.0 (no scaling).\\n            scale_axis_equally: Whether to always scale both axis (x and y)\\n                in the same way. If set to False, then e.g. the Augmenter\\n                might scale the x-axis by 20% and the y-axis by -5%.\\n                Default is False.\\n            rotation_deg: By how much the image may be rotated around its\\n                center (in degrees). The negative rotation will automatically\\n                be derived from this value. E.g. a value of 20 allows any\\n                rotation between -20 degrees and +20 degrees. You may set min\\n                and max values yourself by using a tuple instead, e.g. (5, 20)\\n                to rotate between +5 und +20 degrees. Default is 0 (no\\n                rotation).\\n            shear_deg: By how much the image may be sheared (in degrees). The\\n                negative value will automatically be derived from this value.\\n                E.g. a value of 20 allows any shear between -20 degrees and\\n                +20 degrees. You may set min and max values yourself by using a\\n                tuple instead, e.g. (5, 20) to shear between +5 und +20\\n                degrees. Default is 0 (no shear).\\n            translation_x_px: By up to how many pixels the image may be\\n                translated (moved) on the x-axis. The negative value will\\n                automatically be derived from this value. E.g. a value of +7\\n                allows any translation between -7 and +7 pixels on the x-axis.\\n                You may set min and max values yourself by using a tuple\\n                instead, e.g. (5, 20) to translate between +5 und +20 pixels.\\n                Default is 0 (no translation on the x-axis).\\n            translation_y_px: See translation_x_px, just for the y-axis.\\n            transform_channels_equally: Whether to apply the exactly same\\n                transformations to each channel of an image (True). Setting\\n                it to False allows different transformations per channel,\\n                e.g. the red-channel might be rotated by +20 degrees, while\\n                the blue channel (of the same image) might be rotated\\n                by -5 degrees. If you don\\'t have any channels (2D grayscale),\\n                you can simply ignore this setting.\\n                Default is True (transform all equally).\\n        \"\"\"\\n        self.img_width_px = img_width_px\\n        self.img_height_px = img_height_px\\n        self.channel_is_first_axis = channel_is_first_axis\\n\\n        self.hflip_prob = 0.0\\n        # note: we have to check first for floats, otherwise \"hflip == True\"\\n        # will evaluate to true if hflip is 1.0. So chosing 1.0 (100%) would\\n        # result in hflip_prob to be set to 0.5 (50%).\\n        if isinstance(hflip, float):\\n            assert hflip >= 0.0 and hflip <= 1.0\\n            self.hflip_prob = hflip\\n        elif hflip == True:\\n            self.hflip_prob = 0.5\\n        elif hflip == False:\\n            self.hflip_prob = 0.0\\n        else:\\n            raise Exception(\"Unexpected value for parameter \\'hflip\\'.\")\\n\\n        self.vflip_prob = 0.0\\n        if isinstance(vflip, float):\\n            assert vflip >= 0.0 and vflip <= 1.0\\n            self.vflip_prob = vflip\\n        elif vflip == True:\\n            self.vflip_prob = 0.5\\n        elif vflip == False:\\n            self.vflip_prob = 0.0\\n        else:\\n            raise Exception(\"Unexpected value for parameter \\'vflip\\'.\")\\n\\n        self.scale_to_percent = scale_to_percent\\n        self.scale_axis_equally = scale_axis_equally\\n        self.rotation_deg = rotation_deg\\n        self.shear_deg = shear_deg\\n        self.translation_x_px = translation_x_px\\n        self.translation_y_px = translation_y_px\\n        self.transform_channels_equally = transform_channels_equally\\n        self.cval = 0.0\\n        self.interpolation_order = 1\\n        self.pregenerated_matrices = None\\n\\n    def pregenerate_matrices(self, nb_matrices, seed=None):\\n        \"\"\"Pregenerate/cache augmentation matrices.\\n\\n        If matrices are pregenerated, augment_batch() will reuse them on\\n        each call. The augmentations will not always be the same,\\n        as the order of the matrices will be randomized (when\\n        they are applied to the images). The requirement for that is though\\n        that you pregenerate enough of them (e.g. a couple thousand).\\n\\n        Note that generating the augmentation matrices is usually fast\\n        and only starts to make sense if you process millions of small images\\n        or many tens of thousands of big images.\\n\\n        Each call to this method results in pregenerating a new set of matrices,\\n        e.g. to replace a list of matrices that has been used often enough.\\n\\n        Calling this method with nb_matrices set to 0 will remove the\\n        pregenerated matrices and augment_batch() returns to its default\\n        behaviour of generating new matrices on each call.\\n\\n        Args:\\n            nb_matrices: The number of matrices to pregenerate. E.g. a few\\n                thousand. If set to 0, the matrices will be generated again on\\n                each call of augment_batch().\\n            seed: A random seed to use.\\n        \"\"\"\\n        assert nb_matrices >= 0\\n        if nb_matrices == 0:\\n            self.pregenerated_matrices = None\\n        else:\\n            matrices = create_aug_matrices(nb_matrices,\\n                                           self.img_width_px,\\n                                           self.img_height_px,\\n                                           scale_to_percent=self.scale_to_percent,\\n                                           scale_axis_equally=self.scale_axis_equally,\\n                                           rotation_deg=self.rotation_deg,\\n                                           shear_deg=self.shear_deg,\\n                                           translation_x_px=self.translation_x_px,\\n                                           translation_y_px=self.translation_y_px,\\n                                           seed=seed)\\n            self.pregenerated_matrices = matrices\\n\\n    def augment_batch(self, images, seed=None):\\n        \"\"\"Augments a batch of images.\\n\\n        Applies all settings (rotation, shear, translation, ...) that\\n        have been chosen in the constructor.\\n\\n        Args:\\n            images: Numpy array (dtype: uint8, i.e. values 0-255) with the images.\\n                Expected shape is either (image-index, height, width) for\\n                grayscale images or (image-index, channel, height, width) for\\n                images with channels (e.g. RGB) where the channel has the first\\n                index or (image-index, height, width, channel) for images with\\n                channels, where the channel is the last index.\\n                If your shape is (image-index, channel, width, height) then\\n                you must also set channel_is_first_axis=True in the constructor.\\n            seed: Seed to use for python\\'s and numpy\\'s random functions.\\n                Default is None (dont use a seed).\\n\\n        Returns:\\n            Augmented images as numpy array of dtype float32 (i.e. values\\n            are between 0.0 and 1.0).\\n        \"\"\"\\n        shape = images.shape\\n        nb_channels = 0\\n        if len(shape) == 3:\\n            # shape like (image_index, y-axis, x-axis)\\n            assert shape[1] == self.img_height_px\\n            assert shape[2] == self.img_width_px\\n            nb_channels = 1\\n        elif len(shape) == 4:\\n            if not self.channel_is_first_axis:\\n                # shape like (image-index, y-axis, x-axis, channel-index)\\n                assert shape[1] == self.img_height_px\\n                assert shape[2] == self.img_width_px\\n                nb_channels = shape[3]\\n            else:\\n                # shape like (image-index, channel-index, y-axis, x-axis)\\n                assert shape[2] == self.img_height_px\\n                assert shape[3] == self.img_width_px\\n                nb_channels = shape[1]\\n        else:\\n            msg = \"Mismatch between images shape %s and \" \\\\\\n                  \"predefined image width/height (%d/%d).\"\\n            raise Exception(msg % (str(shape), self.img_width_px, self.img_height_px))\\n\\n        if seed:\\n            random.seed(seed)\\n            np.random.seed(seed)\\n\\n        # --------------------------------\\n        # horizontal and vertical flipping/mirroring\\n        # --------------------------------\\n        # This should be done before applying the affine matrices, as otherwise\\n        # contents of image might already be rotated/translated out of the image.\\n        # It is done with numpy instead of the affine matrices, because\\n        # scikit-image doesn\\'t offer a nice interface to add mirroring/flipping\\n        # to affine transformations. The numpy operations are O(1), so they\\n        # shouldn\\'t have a noticeable effect on runtimes. They also won\\'t suffer\\n        # from interpolation problems.\\n        if self.hflip_prob > 0 or self.vflip_prob > 0:\\n            # TODO this currently ignores the setting in\\n            # transform_channels_equally and will instead always flip all\\n            # channels equally\\n\\n            # if this is simply a view, then the input array gets flipped too\\n            # for some reason\\n            images_flipped = np.copy(images)\\n            #images_flipped = images.view()\\n\\n            if len(shape) == 4 and self.channel_is_first_axis:\\n                # roll channel to the last axis\\n                # swapaxes doesnt work here, because\\n                #  (image index, channel, y, x)\\n                # would be turned into\\n                #  (image index, x, y, channel)\\n                # and y needs to come before x\\n                images_flipped = np.rollaxis(images_flipped, 1, 4)\\n\\n            y_p = self.hflip_prob\\n            x_p = self.vflip_prob\\n            for i in range(images.shape[0]):\\n                if y_p > 0 and random.random() < y_p:\\n                    images_flipped[i] = np.fliplr(images_flipped[i])\\n                if x_p > 0 and random.random() < x_p:\\n                    images_flipped[i] = np.flipud(images_flipped[i])\\n\\n            if len(shape) == 4 and self.channel_is_first_axis:\\n                # roll channel back to the second axis (index 1)\\n                images_flipped = np.rollaxis(images_flipped, 3, 1)\\n            images = images_flipped\\n\\n        # --------------------------------\\n        # if no augmentation has been chosen, stop early\\n        # for improved performance (evade applying matrices)\\n        # --------------------------------\\n        if self.pregenerated_matrices is None \\\\\\n           and self.scale_to_percent == 1.0 and self.rotation_deg == 0 \\\\\\n           and self.shear_deg == 0 \\\\\\n           and self.translation_x_px == 0 and self.translation_y_px == 0:\\n            return np.array(images, dtype=np.float32) / 255\\n\\n        # --------------------------------\\n        # generate transformation matrices\\n        # --------------------------------\\n        if self.pregenerated_matrices is not None:\\n            matrices = self.pregenerated_matrices\\n        else:\\n            # estimate the number of matrices required\\n            if self.transform_channels_equally:\\n                nb_matrices = shape[0]\\n            else:\\n                nb_matrices = shape[0] * nb_channels\\n\\n            # generate matrices\\n            matrices = create_aug_matrices(nb_matrices,\\n                                           self.img_width_px,\\n                                           self.img_height_px,\\n                                           scale_to_percent=self.scale_to_percent,\\n                                           scale_axis_equally=self.scale_axis_equally,\\n                                           rotation_deg=self.rotation_deg,\\n                                           shear_deg=self.shear_deg,\\n                                           translation_x_px=self.translation_x_px,\\n                                           translation_y_px=self.translation_y_px,\\n                                           seed=seed)\\n\\n        # --------------------------------\\n        # apply transformation matrices (i.e. augment images)\\n        # --------------------------------\\n        return apply_aug_matrices(images, matrices,\\n                                  transform_channels_equally=self.transform_channels_equally,\\n                                  channel_is_first_axis=self.channel_is_first_axis,\\n                                  cval=self.cval, interpolation_order=self.interpolation_order,\\n                                  seed=seed)\\n\\n    def plot_image(self, image, nb_repeat=40, show_plot=True):\\n        \"\"\"Plot augmented variations of an image.\\n\\n        This method takes an image and plots it by default in 40 differently\\n        augmented versions.\\n\\n        This method is intended to visualize the strength of your chosen\\n        augmentations (so for debugging).\\n\\n        Args:\\n            image: The image to plot.\\n            nb_repeat: How often to plot the image. Each time it is plotted,\\n                the chosen augmentation will be different. (Default: 40).\\n            show_plot: Whether to show the plot. False makes sense if you\\n                don\\'t have a graphical user interface on the machine.\\n                (Default: True)\\n\\n        Returns:\\n            The figure of the plot.\\n            Use figure.savefig() to save the image.\\n        \"\"\"\\n        if len(image.shape) == 2:\\n            images = np.resize(image, (nb_repeat, image.shape[0], image.shape[1]))\\n        else:\\n            images = np.resize(image, (nb_repeat, image.shape[0], image.shape[1],\\n                               image.shape[2]))\\n        return self.plot_images(images, True, show_plot=show_plot)\\n\\n    def plot_images(self, images, augment, show_plot=True, figure=None):\\n        \"\"\"Plot augmented variations of images.\\n\\n        The images will all be shown in the same window.\\n        It is recommended to not plot too many of them (i.e. stay below 100).\\n\\n        This method is intended to visualize the strength of your chosen\\n        augmentations (so for debugging).\\n\\n        Args:\\n            images: A numpy array of images. See augment_batch().\\n            augment: Whether to augment the images (True) or just display\\n                them in the way they are (False).\\n            show_plot: Whether to show the plot. False makes sense if you\\n                don\\'t have a graphical user interface on the machine.\\n                (Default: True)\\n            figure: The figure of the plot in which to draw the images.\\n                Provide the return value of this function (from a prior call)\\n                to draw in the same plot window again. Chosing \\'None\\' will\\n                create a new figure. (Default is None.)\\n\\n        Returns:\\n            The figure of the plot.\\n            Use figure.savefig() to save the image.\\n        \"\"\"\\n        import matplotlib.pyplot as plt\\n        import matplotlib.cm as cm\\n\\n        if augment:\\n            images = self.augment_batch(images)\\n\\n        # (Lists of) Grayscale images have the shape (image index, y, x)\\n        # Multi-Channel images therefore must have 4 or more axes here\\n        if len(images.shape) >= 4:\\n            # The color-channel is expected to be the last axis by matplotlib\\n            # therefore exchange the axes, if its the first one here\\n            if self.channel_is_first_axis:\\n                images = np.rollaxis(images, 1, 4)\\n\\n        nb_cols = 10\\n        nb_rows = 1 + int(images.shape[0] / nb_cols)\\n        if figure is not None:\\n            fig = figure\\n            plt.figure(fig.number)\\n            fig.clear()\\n        else:\\n            fig = plt.figure(figsize=(10, 10))\\n\\n        for i, image in enumerate(images):\\n            image = images[i]\\n\\n            plot_number = i + 1\\n            ax = fig.add_subplot(nb_rows, nb_cols, plot_number, xticklabels=[],\\n                                 yticklabels=[])\\n            ax.set_axis_off()\\n            # \"cmap\" should restrict the color map to grayscale, but strangely\\n            # also works well with color images\\n            imgplot = plt.imshow(image, cmap=cm.Greys_r, aspect=\"equal\")\\n\\n        # not showing the plot might be useful e.g. on clusters\\n        if show_plot:\\n            plt.show()\\n\\n        return fig\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "files.upload()\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp  /content/drive/MyDrive/2.SocialTeaching/AdvancedComputerVision/BBR/libraries/ImageAugmenter.py .\n",
        "# !cp  /content/drive/MyDrive/2.SocialTeaching/AdvancedComputerVision/BBR/libraries/dataset.py .\n",
        "# # !cp  /content/drive/MyDrive/2.SocialTeaching/AdvancedComputerVision/BBR/libraries/split_by_color.py ."
      ],
      "metadata": {
        "id": "pBlkzOhricm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tvKRYjpBxDe"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import sys\n",
        "CURRENT_DIR = \"/content/drive/MyDrive/Private_ThiGiacMayTinhNangCao/BT_CatFace\"\n",
        "sys.path.append(CURRENT_DIR);\n",
        "\n",
        "from dataset import Dataset\n",
        "import numpy as np\n",
        "import argparse\n",
        "import random\n",
        "import os\n",
        "from scipy import misc\n",
        "from skimage import draw\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "\n",
        "MODEL_IMAGE_HEIGHT = 128\n",
        "MODEL_IMAGE_WIDTH = 128\n",
        "NB_LOAD_IMAGES = 9500\n",
        "SPLIT = 0.1 # Tỉ lệ giữa train và validation là 9 : 1\n",
        "EPOCHS = 150\n",
        "BATCH_SIZE = 64\n",
        "SAVE_WEIGHTS_FILEPATH = os.path.join(CURRENT_DIR, \"cat_face_locator.weights\")\n",
        "SAVE_WEIGHTS_CHECKPOINT_FILEPATH = os.path.join(CURRENT_DIR, \"cat_face_locator.best.weights\")\n",
        "SAVE_PREDICTIONS = True\n",
        "SAVE_PREDICTIONS_DIR = os.path.join(CURRENT_DIR, \"predictions\")\n",
        "DATASET_DIR = \"/content/drive/MyDrive/Private_ThiGiacMayTinhNangCao/CatData\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZzRjDlhJH2z"
      },
      "source": [
        "# BƯỚC 4: LOAD DỮ LIỆU ẢNH VÀ ANNOTATION\n",
        "\n",
        "Bài tập này đã chuẩn bị thư viện dataset để phục vụ cho việc load một dataset lên và các thao tác hỗ trợ load theo batch. Nếu bạn quan tâm thì có thể đọc kỹ cài đặt của lớp đối tượng Dataset này."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFn0e37DHLjh",
        "outputId": "a006c205-366d-4b9d-bfd4-e88cd7ebcf15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmdir: failed to remove '/content/drive/MyDrive/Private_ThiGiacMayTinhNangCao/CatData/.ipynb_checkpoints': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rmdir \"/content/drive/MyDrive/Private_ThiGiacMayTinhNangCao/CatData/.ipynb_checkpoints\"\n",
        "# initialize dataset\n",
        "subdir_names = [\"CAT_00\", \"CAT_01\", \"CAT_02\", \"CAT_03\", \"CAT_04\", \"CAT_05\", \"CAT_06\"]\n",
        "subdirs = [os.path.join(DATASET_DIR, subdir) for subdir in subdir_names]\n",
        "# dataset = tf.keras.utils.image_dataset_from_directory(DATASET_DIR)\n",
        "dataset = Dataset(subdirs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBF2LaifL4Hh",
        "outputId": "d5c197df-2dfb-4c75-b887-d23bf2c19387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/Private_ThiGiacMayTinhNangCao/CatData/CAT_00', '/content/drive/MyDrive/Private_ThiGiacMayTinhNangCao/CatData/CAT_01', '/content/drive/MyDrive/Private_ThiGiacMayTinhNangCao/CatData/CAT_02', '/content/drive/MyDrive/Private_ThiGiacMayTinhNangCao/CatData/CAT_03', '/content/drive/MyDrive/Private_ThiGiacMayTinhNangCao/CatData/CAT_04', '/content/drive/MyDrive/Private_ThiGiacMayTinhNangCao/CatData/CAT_05', '/content/drive/MyDrive/Private_ThiGiacMayTinhNangCao/CatData/CAT_06']\n"
          ]
        }
      ],
      "source": [
        "print(subdirs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOqJNR5dJeuY"
      },
      "source": [
        "# BƯỚC 5: CHUẨN BỊ MỘT SỐ HÀM CƠ BẢN\n",
        "\n",
        "Một số hàm cơ bản phục vụ cho quá trình huấn luyện như:\n",
        "- load_xy: để load toàn bộ dữ liệu và tạo dữ liệu groundtruth dựa trên các điểm đặc trưng trên mặt con mèo.\n",
        "- unnormalize_prediction: biến đổi vector output đầu ra thành dạng toạ độ ảnh hệ toạ độ nguyên theo pixel. Bước này là để khôi phục hệ toạ độ bounding box từ dạng đã chuẩn hoá (normalized) có miền giá trị 0-1 sang hệ toạ độ theo pixel của ảnh trong tập dữ liệu.\n",
        "- draw_predicted_rectangle: vẽ bounding box dự đoán từ hệ thống lên ảnh. Bước này bao gồm việc chuẩn hoá ngược sang hệ pixel và vẽ lên trên ảnh màu.\n",
        "- Clip: để đảm bảo rằng toạ độ đều không âm và không vượt quá khuôn khổ ảnh.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Av_z2AACEYTO"
      },
      "outputs": [],
      "source": [
        "def load_xy(dataset, nb_load):\n",
        "    \"\"\"Loads X and y (examples with labels) for the dataset.\n",
        "    Examples are images.\n",
        "    Labels are the coordinates of the face rectangles with their half-heights and half-widths\n",
        "    (each normalized to 0-1 with respect to the image dimensions.)\n",
        "\n",
        "    Args:\n",
        "        dataset            The Dataset object.\n",
        "        nb_load            Intended number of images to load.\n",
        "    Returns:\n",
        "        X (numpy array of shape (N, height, width, 3)),\n",
        "        y (numpy array of shape (N, 4))\n",
        "    \"\"\"\n",
        "    i = 0\n",
        "    nb_images = min(nb_load, len(dataset.fps))\n",
        "    X = np.zeros((nb_images, MODEL_IMAGE_HEIGHT, MODEL_IMAGE_WIDTH, 3), dtype=np.float32)\n",
        "    y = np.zeros((nb_images, 4), dtype=np.float32)\n",
        "\n",
        "    for img_idx, image in enumerate(dataset.get_images()):\n",
        "        if img_idx % 100 == 0:\n",
        "            print(\"Loading image %d of %d...\" % (img_idx+1, nb_images))\n",
        "        image.resize(MODEL_IMAGE_HEIGHT, MODEL_IMAGE_WIDTH)\n",
        "        # Norm image\n",
        "        X[i] = image.to_array() / 255.0\n",
        "        # Prepare groundtruth\n",
        "        face_rect = image.keypoints.get_rectangle(image)\n",
        "        face_rect.normalize(image)\n",
        "        center = face_rect.get_center()\n",
        "        width = face_rect.get_width() / 2\n",
        "        height = face_rect.get_height() / 2\n",
        "        y[i] = [center.y, center.x, height, width]\n",
        "        \n",
        "        i += 1\n",
        "        if i >= nb_images:\n",
        "            break\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def unnormalize_prediction(y, x, half_height, half_width, \\\n",
        "                           img_height=MODEL_IMAGE_HEIGHT, img_width=MODEL_IMAGE_WIDTH):\n",
        "    \"\"\"Transforms a predictions from normalized (0 to 1) y, x, half-width,\n",
        "    half-height to pixel values (top left y, top left x, bottom right y,\n",
        "    bottom right x).\n",
        "    Args:\n",
        "        y: Normalized y coordinate of rectangle center.\n",
        "        x: Normalized x coordinate of rectangle center.\n",
        "        half_height: Normalized height of rectangle.\n",
        "        half_width: Normalized width of rectangle.\n",
        "        img_height: Height of the image to use while unnormalizing.\n",
        "        img_width: Width of the image to use while unnormalizing.\n",
        "    Returns:\n",
        "        (top left y in px, top left x in px, bottom right y in px,\n",
        "        bottom right x in px)\n",
        "    \"\"\"\n",
        "    # calculate x, y of corners in pixels\n",
        "    tl_y = int((y - half_height) * img_height)\n",
        "    tl_x = int((x - half_width) * img_width)\n",
        "    br_y = int((y + half_height) * img_height)\n",
        "    br_x = int((x + half_width) * img_width)\n",
        "\n",
        "    # make sure that x and y coordinates are within image boundaries\n",
        "    tl_y = clip(0, tl_y, img_height-2)\n",
        "    tl_x = clip(0, tl_x, img_width-2)\n",
        "    br_y = clip(0, br_y, img_height-1)\n",
        "    br_x = clip(0, br_x, img_width-1)\n",
        "\n",
        "    # make sure that top left corner is really top left of bottom right values\n",
        "    if tl_y > br_y:\n",
        "        tl_y, br_y = br_y, tl_y\n",
        "    if tl_x > br_x:\n",
        "        tl_x, br_x = br_x, tl_x\n",
        "\n",
        "    # make sure that the area covered is at least 1px,\n",
        "    # move preferably the top left corner\n",
        "    # but dont move it outside of the image\n",
        "    if tl_y == br_y:\n",
        "        if tl_y == 0:\n",
        "            br_y += 1\n",
        "        else:\n",
        "            tl_y -= 1\n",
        "\n",
        "    if tl_x == br_x:\n",
        "        if tl_x == 0:\n",
        "            br_x += 1\n",
        "        else:\n",
        "            tl_x -= 1\n",
        "\n",
        "    return tl_y, tl_x, br_y, br_x\n",
        "\n",
        "def draw_predicted_rectangle(image_arr, y, x, half_height, half_width):\n",
        "    \"\"\"Draws a rectangle onto the image at the provided coordinates.\n",
        "    Args:\n",
        "        image_arr: Numpy array of the image.\n",
        "        y: y-coordinate of the rectangle (normalized to 0-1).\n",
        "        x: x-coordinate of the rectangle (normalized to 0-1).\n",
        "        half_height: Half of the height of the rectangle (normalized to 0-1).\n",
        "        half_width: Half of the width of the rectangle (normalized to 0-1).\n",
        "    Returns:\n",
        "        Modified image (numpy array)\n",
        "    \"\"\"\n",
        "    #assert image_arr.shape[0] == 3, str(image_arr.shape)\n",
        "    #height = image_arr.shape[1]\n",
        "    #width = image_arr.shape[2]\n",
        "    height = image_arr.shape[0]\n",
        "    width = image_arr.shape[1]\n",
        "    tl_y, tl_x, br_y, br_x = unnormalize_prediction(y, x, half_height, half_width, \\\n",
        "                                                    img_height=height, img_width=width)\n",
        "    image_arr = np.copy(image_arr) * 255\n",
        "    #image_arr = np.rollaxis(image_arr, 0, 3)\n",
        "    return draw_rectangle(image_arr, tl_y, tl_x, br_y, br_x)\n",
        "\n",
        "def draw_rectangle(img, tl_y, tl_x, br_y, br_x):\n",
        "    \"\"\"Draws a rectangle onto an image.\n",
        "    Args:\n",
        "        img: The image as a numpy array of shape (row, col, channel).\n",
        "        tl_y: Top left y coordinate as pixel.\n",
        "        tl_x: Top left x coordinate as pixel.\n",
        "        br_y: Top left y coordinate as pixel.\n",
        "        br_x: Top left x coordinate as pixel.\n",
        "    Returns:\n",
        "        image with rectangle\n",
        "    \"\"\"\n",
        "    assert img.shape[2] == 3, img.shape[2]\n",
        "    img = np.copy(img)\n",
        "    lines = [\n",
        "        (tl_y, tl_x, tl_y, br_x), # top left to top right\n",
        "        (tl_y, br_x, br_y, br_x), # top right to bottom right\n",
        "        (br_y, br_x, br_y, tl_x), # bottom right to bottom left\n",
        "        (br_y, tl_x, tl_y, tl_x)  # bottom left to top left\n",
        "    ]\n",
        "    for y0, x0, y1, x1 in lines:\n",
        "        rr, cc, val = draw.line_aa(y0, x0, y1, x1)\n",
        "        img[rr, cc, 0] = val * 255\n",
        "\n",
        "    return img\n",
        "\n",
        "def clip(lower, val, upper):\n",
        "    \"\"\"Clips a value. For lower bound L, upper bound U and value V it\n",
        "    makes sure that L <= V <= U holds.\n",
        "    Args:\n",
        "        lower: Lower boundary (including)\n",
        "        val: The value to clip\n",
        "        upper: Upper boundary (including)\n",
        "    Returns:\n",
        "        value within bounds\n",
        "    \"\"\"\n",
        "    if val < lower:\n",
        "        return lower\n",
        "    elif val > upper:\n",
        "        return upper\n",
        "    else:\n",
        "        return val\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFVYevSrK1mh"
      },
      "source": [
        "# BƯỚC 6: TẠO DỰNG MÔ HÌNH\n",
        "\n",
        "Đây là phần việc chính mà các bạn cần thực hiện để cài đặt mô hình phát hiện mặt mèo trong ảnh. Tổng thể sẽ có 4 bước (như slide bài giảng):\n",
        "\n",
        "Bước 1: Train mô phân lớp đối tượng (hoặc sử dụng pre-trained model)\n",
        "\n",
        "Bước 2: Nối lớp cuối cùng của lớp Convolution với một mạng Neural để ước lượng (regression) toạ độ bounding box xung quanh mặt một con mèo. Lưu ý đầu ra sẽ là một vector 4 chiều tương ứng với tâm và một nửa bề ngang, bề cao của bounding box.\n",
        "\n",
        "Bước 3: Huấn luyện mạng Bounding Box Regressor. Lưu ý rằng ta sẽ không huấn luyện các lớp convolution trước đó nữa.\n",
        "\n",
        "Bước 4: Kết hợp định vị và phân lớp để loại bỏ những ảnh không chữa mèo trong đó."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUB6GQ9pIonc",
        "outputId": "e2ab8eab-3b65-4334-8efb-7609e183396f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 128, 128, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 128, 128, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 64, 64, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 64, 64, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 32, 32, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 32, 32, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 32, 32, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 16, 16, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 16, 16, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 8, 8, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "vgg_conv.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2ZLoI2P2pSS"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_model(image_height, image_width, loss, optimizer):\n",
        "    \"\"\"Creates the cat face locator model.\n",
        "\n",
        "    Args:\n",
        "        image_height: The height of the input images.\n",
        "        image_width: The width of the input images.\n",
        "        loss: Keras loss function (name or object), e.g. \"mse\".\n",
        "        optimizer: Keras optimizer to use, e.g. Adam() or \"sgd\".\n",
        "    Returns:\n",
        "        Sequential\n",
        "    \"\"\"\n",
        "    # STEP 1:\n",
        "    # Load VGG16 model pretrained on ImageNet dataset excluding fully connected layers (Classification Head) (include_top = False)\n",
        "    vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_height, image_width, 3))\n",
        "\n",
        "\n",
        "    # STEP 2: Attach Regression Head into pre-trained convolution layers\n",
        "    # Freeze the layers except the last 4 layers\n",
        "    # CODE: for each layer in vgg_conv.layer --> set 'trainable' = False\n",
        "    \n",
        "    for layer in vgg_conv.layers[:-4]:\n",
        "        layer.trainable = False\n",
        "    for layer in vgg_conv.layers[-4:]:\n",
        "        vgg_conv.trainable = True\n",
        "\n",
        "\n",
        "    # Create the model\n",
        "    model = Sequential()\n",
        "    # CODE: Add vgg_conv to the model\n",
        "    # ... model.add(vgg_conv)\n",
        "    model.add(vgg_conv)\n",
        "    \n",
        "\n",
        "    # After adding vgg_conv, the last tensor should be flatten to be a vector\n",
        "    model.add(Flatten())\n",
        "\n",
        "\n",
        "    # CODE: Add bounding box Regression Head here.\n",
        "    # Please choose number of neuron in each layer and suitable activation functions\n",
        "    # You may use Dropout for generalization\n",
        "    # Adding 3 layers, 4096 neurons -> ReLU-> 4096 neurons -> ReLU 4 neurons\n",
        "    # ... model.add(Dense(4096, 'relu'))\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dense(4096, activation ='relu'))\n",
        "    model.add(Dense(4, activation='relu'))\n",
        "\n",
        "\n",
        "    # Compile with mean squared error\n",
        "    print(\"Compiling...\")\n",
        "    model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn9ypTDANdEM"
      },
      "source": [
        "Tiếp theo ta sẽ load dữ liệu và chia ra làm 2 phần: train và validation. Bước này hơi lâu nên cần chạy kiên nhẫn một chút."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "0gSPEURdEZa1",
        "outputId": "f80e69c1-b6f8-43e5-b61e-710c45d90d15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading images...\n",
            "Loading image 1 of 9500...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-f6d0099006f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load images and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading images...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNB_LOAD_IMAGES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# split train and val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-1b9228acda6e>\u001b[0m in \u001b[0;36mload_xy\u001b[0;34m(dataset, nb_load)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Prepare groundtruth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mface_rect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeypoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mface_rect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mcenter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_rect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_center\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dataset.py\u001b[0m in \u001b[0;36mget_rectangle\u001b[0;34m(self, image, method)\u001b[0m\n\u001b[1;32m    670\u001b[0m             \u001b[0;31m# rectangle 4: like 3, but squared with Rectangle.square()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m             \u001b[0mrect3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             \u001b[0mrect3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrect3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dataset.py\u001b[0m in \u001b[0;36mget_rectangle\u001b[0;34m(self, image, method)\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;31m# the corners of both rectangles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m             \u001b[0mrect0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m             \u001b[0mrect2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dataset.py\u001b[0m in \u001b[0;36mget_rectangle\u001b[0;34m(self, image, method)\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0;31m# rectangle 0: bounding box around provided keypoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mRectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_min_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_min_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;31m# rectangle 1: the same rectangle as rect 0, but translated to the center of the face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tl_y, tl_x, br_y, br_x, is_normalized)\u001b[0m\n\u001b[1;32m    822\u001b[0m                             pixel values\"\"\"\n\u001b[1;32m    823\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtl_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbr_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbr_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mtl_y\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtl_x\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbr_y\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbr_x\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtl_y\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbr_y\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtl_x\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbr_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_normalized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# load images and labels\n",
        "print(\"Loading images...\")\n",
        "X, y = load_xy(dataset, NB_LOAD_IMAGES)\n",
        "\n",
        "# split train and val\n",
        "nb_images = X.shape[0]\n",
        "nb_train = int(nb_images * (1 - SPLIT))\n",
        "X_train = X[0:nb_train, ...]\n",
        "y_train = y[0:nb_train, ...]\n",
        "X_val = X[nb_train:, ...]\n",
        "y_val = y[nb_train:, ...]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6UfsnMkNrip"
      },
      "source": [
        "Bắt đầu, tạo mô hình với độ lỗi sử dụng độ đo L2 và thuật toán Adam Optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fe18uv8gd1Y"
      },
      "outputs": [],
      "source": [
        "# You can change loss and optimizer if you want\n",
        "print(\"Creating model...\")\n",
        "model = create_model(MODEL_IMAGE_HEIGHT, MODEL_IMAGE_WIDTH, \"mse\", Adam())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0aE-maON0tC"
      },
      "source": [
        "Bắt đầu quá trình huấn luyện và lưu các checkpoint trong quá trình train. Sau khi train hết 150 epoch ta sẽ tiến hành quá trình test trên tập ảnh validation và lưu kết quả vào thư mục Predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DNT4Bbx5jg9"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo4uoda8Eg6y"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "#  STEP 3: Train with Regression Head\n",
        "# Fit data to model\n",
        "checkpoint_cb = ModelCheckpoint(SAVE_WEIGHTS_CHECKPOINT_FILEPATH, verbose=1, \\\n",
        "                                save_best_only=True)\n",
        "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.0,\n",
        "          validation_data=(X_val, y_val),\n",
        "          callbacks=[checkpoint_cb])\n",
        "\n",
        "# save weights\n",
        "print(\"Saving weights...\")\n",
        "model.save_weights(SAVE_WEIGHTS_FILEPATH, overwrite=True)\n",
        "\n",
        "#model.load_weights(SAVE_WEIGHTS_CHECKPOINT_FILEPATH)\n",
        "\n",
        "# save predictions on val set\n",
        "if SAVE_PREDICTIONS:\n",
        "    print(\"Saving example predictions...\")\n",
        "    y_preds = model.predict(X_val, batch_size=BATCH_SIZE)\n",
        "    for img_idx, (y, x, half_height, half_width) in enumerate(y_preds):\n",
        "        img_arr = draw_predicted_rectangle(X_val[img_idx], y, x, half_height, half_width)\n",
        "        filepath = os.path.join(SAVE_PREDICTIONS_DIR, \"%d.png\" % (img_idx,))\n",
        "        cv2.imwrite(filepath, np.squeeze(img_arr))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-mFI9vmOEgT"
      },
      "source": [
        "Sau khi đã huấn luyện xong Bounding Box Regressor. Bạn hãy kết hợp với mạng ban đầu để xác định xem mức độ tin cậy của bounding box tìm được bởi mô hình có mức độ tin cậy là bao nhiêu. Nếu trên ngưỡng thì ta giữ lại, còn lại thì sẽ loại bỏ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb723X2vOMjn"
      },
      "outputs": [],
      "source": [
        "# Step 4\n",
        "# CODE: Apply Classification Head and Regression Head for a full system of localization\n",
        "\n",
        "def check_IoU(img, model, bbox_true, thresh=0.5):\n",
        "    (tl_y, tl_x, br_y, br_x) = bbox_true\n",
        "    y_pred = model.predict(np.expand_dims(img, axis=0)) \n",
        "    (y, x, half_height, half_width) = y_pred\n",
        "    (tl_y_pred, tl_x_pred, br_y_pred, br_x_pred) = unnormalize_prediction(y, x, half_height, half_width, img_height=img.shape[0], img_width=img.shape[1])\n",
        "\n",
        "    xA = max(tl_x, tl_x_pred)\n",
        "    yA = min(tl_y, tl_y_pred)\n",
        "    yB = max(br_y, br_y_pred)\n",
        "    xB = min(br_x, br_x_pred)\n",
        "\n",
        "    interArea = abs(max((xB - xA, 0)) * max((yB - yA), 0))\n",
        "    if interArea == 0:\n",
        "        return 0\n",
        "\n",
        "    boxAArea = abs((br_x - tl_x) * (tl_y - br_y))\n",
        "    boxBArea = abs((br_x_pred - tl_x_pred) * (br_y_pred - tl_y_pred))\n",
        "\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "\n",
        "    return iou >= thresh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHmmNpsdLr3L"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypoxeW8AL9Fs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "19521388_HoangTienDung_Cat_face_localization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}